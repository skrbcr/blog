<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>連想記憶モデル（Hopfield network）の実装 | 今月の物置</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="連想記憶モデル（Hopfield network）の実装" />
<meta name="author" content="今月の新刊（@skrbcr）" />
<meta property="og:locale" content="ja" />
<meta name="description" content="（一応 physics ジャンルの投稿です）" />
<meta property="og:description" content="（一応 physics ジャンルの投稿です）" />
<link rel="canonical" href="http://localhost:4000/blog/physics/associative_memory" />
<meta property="og:url" content="http://localhost:4000/blog/physics/associative_memory" />
<meta property="og:site_name" content="今月の物置" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-11-22T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="連想記憶モデル（Hopfield network）の実装" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"今月の新刊（@skrbcr）"},"dateModified":"2023-11-22T00:00:00+09:00","datePublished":"2023-11-22T00:00:00+09:00","description":"（一応 physics ジャンルの投稿です）","headline":"連想記憶モデル（Hopfield network）の実装","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/physics/associative_memory"},"url":"http://localhost:4000/blog/physics/associative_memory"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="今月の物置" /><!--アイコン-->
<link rel="shortcut icon" href="/blog/assets/favicon/favicon.png" type="image/x-icon">

<!--KaTeX-->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js" integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            // customised options
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            macros: {
                "\\arsinh": "\\text{ar}\\!\\sinh",
                "\\arcosh": "\\text{ar}\\!\\cosh",
                "\\artanh": "\\text{ar}\\!\\tanh",
                "\\dd": "\\,d",
                "\\dv": "\\frac{d #1}{d #2}",
                "\\ddv": "\\frac{d^2 #1}{d #2^2}",
                "\\pdv": "\\frac{\\partial #1}{\\partial #2}",
                "\\ppdv": "\\frac{\\partial^{#1} #2}{\\partial #3^{#1}}",
            },
            throwOnError : false,
        });
    });
</script>

<!--CSS-->
<link rel="stylesheet" href="/blog/assets/css/style.css"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>連想記憶モデル（Hopfield network）の実装 | 今月の物置</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="連想記憶モデル（Hopfield network）の実装" />
<meta name="author" content="今月の新刊（@skrbcr）" />
<meta property="og:locale" content="ja" />
<meta name="description" content="（一応 physics ジャンルの投稿です）" />
<meta property="og:description" content="（一応 physics ジャンルの投稿です）" />
<link rel="canonical" href="http://localhost:4000/blog/physics/associative_memory" />
<meta property="og:url" content="http://localhost:4000/blog/physics/associative_memory" />
<meta property="og:site_name" content="今月の物置" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-11-22T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="連想記憶モデル（Hopfield network）の実装" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"今月の新刊（@skrbcr）"},"dateModified":"2023-11-22T00:00:00+09:00","datePublished":"2023-11-22T00:00:00+09:00","description":"（一応 physics ジャンルの投稿です）","headline":"連想記憶モデル（Hopfield network）の実装","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/physics/associative_memory"},"url":"http://localhost:4000/blog/physics/associative_memory"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">今月の物置</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">このサイトについて</a><a class="page-link" href="/blog/categoris/">カテゴリー</a><a class="page-link" href="https://skrbcr.github.io">今月の新刊</a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">連想記憶モデル（Hopfield network）の実装</h1>
    <p class="post-meta">
      投稿日:
      <time class="dt-published" datetime="2023-11-22T00:00:00+09:00" itemprop="datePublished">2023年11月22日
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>（一応 physics ジャンルの投稿です）</p>

<p>（細かい話に興味ない人は「原理」の節を飛ばしてください）</p>

<h2 id="投稿の概要">投稿の概要</h2>
<p>連想記憶モデル（Hopfield model）は人工知能（AI）の一種で、情報の記憶、取り出しを行うことができる。以下で述べるように、不鮮明な入力から記憶した情報を思い出すことができる。本投稿は連想記憶モデルの原理について述べ、簡単な実験を行えるプログラムを紹介する。実験結果についても簡単に述べる。なお、"Statistical Physics of Spin Glasses and Information Processing An Introduction" by Hidetoshi Nishimori<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> を参考文献としてお送りする。</p>

<h2 id="連想記憶モデルとは">連想記憶モデルとは？</h2>
<p>連想記憶モデル（Hopfield network）は情報の記憶・思い出しを行うニューラルネットワーク。例えば、1000枚の画像をこのネットワークに記憶させる。次に、1000枚の中から1枚を選び、それにノイズをのせた画像を用意する。そして、その画像をネットワークに入力すると、徐々に元の画像が再現される。この思い出しは常に成功するのではなく、条件を満たす必要がある。</p>

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UE2mZNWXd-A?si=IULMuAeYakfd99YU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<p>画像を思い出す様子</p>
</center>

<p>連想記憶は脳が記憶したり思い出したりする仕組みを参考に構築されている。そのため、各記憶素子を「ニューロン」と呼ぶ。実際の脳のニューロンは発火したりしなかったりすることで情報を保持・伝達する。本モデルでもニューロンは発火・非発火により情報の保持・伝達を行う。</p>

<h2 id="原理">原理</h2>
<h3 id="パターンの記憶">パターンの記憶</h3>

<p>記憶するパターン数を $p$ とし、各ニューロンの記憶パターンを ${\xi_i^\mu}$ で表す。$\mu$ 番目の記憶パターンでは $i$ 個目のニューロンが $\xi_i^\mu$ の状態（発火・非発火）を表す：</p>

\[\begin{equation}
    \xi_i^{\mu} = \begin{cases}
        1  &amp; \text{：発火} \\
        -1 &amp; \text{：非発火} \\
    \end{cases}
\end{equation}\]

<p>ニューロン同士の相互作用として次の量$J_{ij}$を定義する。系は$J_{ij}$を計算することで $p$ 個のパターンがネットワークを記憶（埋め込み）する：</p>

\[\begin{equation}
    J_{ij} = \frac{1}{N} \sum_{\mu = 1}^{p} \xi_i^\mu \xi_j^\mu
\end{equation}\]

<p>$N$ はニューロンの個数。$J$ は $N\times N$ の行列。また、$J_{ii} = 0$ とする。</p>

<h3 id="ニューロン間のやりとり">ニューロン間のやりとり</h3>

<p>ニューロン同士は情報のやり取りを行うことで記憶したものを思い出す。あるニューロンの状態は他のニューロンの状態により決まる。ニューロン $i$ がニューロン $j$ から受け取る信号の大きさを次のように決める：</p>

\[\begin{equation}
    J_{ij}(S_j + 1) = \begin{cases}
        2J_{ij} &amp; :~S_j = +1 \\
        0       &amp; :~S_j = -1 \\
    \end{cases}
\end{equation}\]

<p>こうすると良い理由は後述。すると、ニューロン $i$ に入ってくる信号の和 $h_i$ は</p>

\[\begin{equation}
    h_i = \sum_{j = 1}^{N} J_{ij}(S_j + 1)
\end{equation}\]

<p>ニューロン $i$ は入力の和があるしきい値 $\theta_i$ を超えるときに発火、そうでなければ非発火とする：</p>

\[\begin{align}
    S_i(t + \Delta t) &amp;= \mathrm{sgn}(h_i - \theta_i) \nonumber\\
                      &amp;= \mathrm{sgn}\left(\sum_{j = 1}^{N} J_{ij}(S_j + 1) - \theta_i\right)
\end{align}\]

<p>ここで $\mathrm{sgn}(\cdot)$ は符号関数。簡単のために、$\theta_i = J_{ij}$ として</p>

\[\begin{equation}
    S_i(t + \Delta t) = \mathrm{sgn}\left(\sum_{j = 1}^{N} J_{ij}S_j(t)\right)
\end{equation}\]

<h3 id="思い出し">思い出し</h3>
<p>ここまで、ニューロンを使った記憶の方法・情報のやり取りの方法を説明した。この方法で本当に上手くいくのか？　上手くいくことを確認する。</p>

<p>思い出すとは、記憶したパターンに近いものを与えられたとき、元のパターンを復元できること。まずは、与えられるニューロンパターンが $\mu$ 番目パターンそのものの場合を考える。時刻 $t$ でニューロンたちが $\mu$ 番目のパターンそのものであるとき、時刻 $t + \Delta t$ では</p>

\[\begin{align}
    S_i (t + \Delta t) &amp;= \mathrm{sgn}\left(\sum_{j = 1}^{N} J_{ij}S_j(t)\right) \nonumber\\
                       &amp;= \mathrm{sgn}\left(\sum_{j = 1}^{N} J_{ij}\xi_j^\mu\right) \nonumber\\
                       &amp;= \mathrm{sgn}\left(\sum_{j = 1}^{N} \frac{1}{N}\sum_{\nu = 1}^p \xi_i^\nu \xi_j^\nu \xi_j^\mu\right) \nonumber\\
                       &amp;= \mathrm{sgn}\left(\sum_{\nu = 1}^p \xi_i^\nu \frac{1}{N} \sum_{j = 1}^{N} \xi_j^\nu \xi_j^\mu\right)
\end{align}\]

<p>ここで</p>

\[\begin{equation}
    \frac{1}{N} \sum_{j = 1}^{N} \xi_j^\nu \xi_j^\mu
\end{equation}\]

<p>は、$\mu = \nu$ のときは 1、$\mu \neq \nu$ のときは $N$ が十分大きければ 0 になりそう。なので、この部分はクロネッカーのデルタ $\delta_{\nu,\mu}$ になる。</p>

\[\begin{align}
    S_i (t + \Delta t) &amp;= \mathrm{sgn}\left(\sum_{\nu = 1}^p \xi_i^\nu \delta_{\nu,\mu}\right) \nonumber\\
                       &amp;= \mathrm{sgn}(\xi_i^\mu) \nonumber\\
                       &amp;= \xi_i^\mu 
\end{align}\]

<p>確かに、時刻 $t + \Delta t$ でも系の状態はパターン $\mu$ のままであった。</p>

<p>次に、与えられるパターンがパターン $\mu$ から少しだけずれているときを考える。これは、系のエネルギーから定性的に説明できる。ニューロン $i$ に関して、入力信号 $h_i$ と状態 $S_i$ の積を考えると</p>

\[\begin{align}
    -h_iS_i &amp;= -\sum_j J_{ij}(S_j + 1) S_i \nonumber\\
            &amp;= -\sum_j J_{ij}S_iS_j + const. \times S_i
\end{align}\]

<p>全ての $i$ について和をとれば</p>

\[\begin{equation}
    H = -\frac{1}{2}\sum_{i,j} J_{ij}S_iS_j
\end{equation}\]

<p>ただし、定数部分は適当。この $H$ に関して次のことがわかる：</p>

<ul>
  <li>$H$ は Ising model の Hamiltonian</li>
  <li>ひとつのニューロンに関して、入力信号と状態が等しいときエネルギーは小さく、異なるときエネルギーは大きくなる</li>
  <li>系の状態がパターンの状態と等しいとき、エネルギーは極小（各ニューロン、入力 $h_i$ と状態 $S_i$ は等しくなっているので）</li>
</ul>

<p>従って、系の状態がパターン $\mu$ から少し異なるとき、エネルギーが極小に落ち込むように、パターン $\mu$ に近づいていく。</p>

<p>以上により、連想記憶モデルは記憶と思い出しが行えることがわかった。</p>

<h2 id="実装">実装</h2>
<p><a href="https://github.com/skrbcr/toybox/tree/main/Hopfield_network">GitHub</a> で公開している。</p>

<p>ここでは簡単に、記憶パートと思い出しパートを取り出して説明する。とはいえ、どちらも行列・ベクトルの計算をするだけで、それには <a href="https://www.boost.org/">Boost C++ Library</a> の <code class="language-plaintext highlighter-rouge">boost::numeric::ublas</code> を用いた。</p>

<p><strong>記憶</strong></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define sgn(x) (x &gt;= 0 ? 1 : -1)  // 符号関数
</span>
<span class="n">std</span><span class="o">::</span><span class="n">mt19937</span> <span class="nf">engine</span><span class="p">(</span><span class="mi">12345</span><span class="p">);</span>  <span class="c1">// 乱数生成器</span>

<span class="n">boost</span><span class="o">::</span><span class="n">numeric</span><span class="o">::</span><span class="n">ublas</span><span class="o">::</span><span class="n">matrix</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">J</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>  <span class="c1">// J 行列</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">boost</span><span class="o">::</span><span class="n">numeric</span><span class="o">::</span><span class="n">ublas</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&gt;</span> <span class="n">xi</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ublas</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">n</span><span class="p">));</span>  <span class="c1">// p個のパターン ξ ベクトル</span>

<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">mu</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">mu</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">;</span> <span class="o">++</span><span class="n">mu</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// p個のパターンに対して</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">mu</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="n">xi</span><span class="p">[</span><span class="n">mu</span><span class="p">](</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">sgn</span><span class="p">((</span><span class="kt">int</span><span class="p">)</span><span class="n">engine</span><span class="p">());</span>  <span class="c1">// (p - 1)個のパターンをランダム生成</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">i</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// 行列の下三角だけ埋める</span>
            <span class="n">J</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="o">+=</span> <span class="n">xi</span><span class="p">[</span><span class="n">mu</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">xi</span><span class="p">[</span><span class="n">mu</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>  <span class="c1">// ξ_i^μ × ξ_j^μ</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">J</span> <span class="o">/=</span> <span class="p">(</span><span class="kt">double</span><span class="p">)</span><span class="n">n</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">i</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="n">J</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">J</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">);</span>  <span class="c1">// 行列の上半分は下半分のコピー（J_ij = J_ji）</span>
</code></pre></div></div>

<p><strong>思い出し</strong></p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">boost</span><span class="o">::</span><span class="n">numeric</span><span class="o">::</span><span class="n">ublas</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span> <span class="n">s</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">stemp</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>  <span class="c1">// s: 入力（元画像にノイズがのったもの）; stemp: 計算の途中結果を一次保存</span>

<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">duration</span><span class="p">;</span> <span class="o">++</span><span class="n">t</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// sgn(J * s)</span>
    <span class="n">stemp</span> <span class="o">=</span> <span class="n">ublas</span><span class="o">::</span><span class="n">prod</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">s</span><span class="p">);</span>  
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="n">s</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">sgn</span><span class="p">(</span><span class="n">stemp</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="結果">結果</h2>
<p>下の画像と、ランダム生成される $p - 1$ 枚の画像を記憶させる（サイズ：100 x 100）。</p>

<p><img src="/blog/assets/2023-11-01-associative_memory/github.png" alt="記憶する画像" title="記憶する画像" /></p>

<p>思い出す画像は、この画像である。画像にノイズをのせてAI に与える。パターン数 $p$ と一致率 $r_n$（$1 - $(ノイズ比率)）を変えて思い出せるかどうかの実験を行った。</p>

<ul>
  <li>$p = 100, n_r = 0.6$</li>
</ul>
<center><video controls="" src="/blog/assets/2023-11-01-associative_memory/p100_n40.mp4" width="50%"></video></center>
<p>あっという間に、完全に画像を復元できた。</p>

<ul>
  <li>$p = 1000, n_r = 0.75$</li>
</ul>
<center><video controls="" src="/blog/assets/2023-11-01-associative_memory/p1000_n25.mp4" width="50%"></video></center>
<p>パターン数を増やすと、再現はできているものの若干のノイズが残る。</p>

<ul>
  <li>$p = 1000, n_r = 0.65$</li>
</ul>
<center><video controls="" src="/blog/assets/2023-11-01-associative_memory/p1000_n35.mp4" width="50%"></video></center>
<p>同じくパターン数1000だが一致率を下げた。すると、再現できなくなった。</p>

<ul>
  <li>$p = 3000, n_r = 0.95$</li>
</ul>
<center><video controls="" src="/blog/assets/2023-11-01-associative_memory/p3000_n5.mp4" width="50%"></video></center>
<p>パターン数を増やしていくと、高い一致率の画像を与えても再現できなくなった。ノイズは減るどころか増えている。</p>

<p>どれくらいのパターン数を埋め込めるのかは、数値計算で特定されている。パターン数 $p$ とニューロン数 $N$ の比率 $\alpha = p / N$ が 0.138 を超えると画像の再現ができなくなるらしい。一致率の方はどういう研究がされているのか、残念ながら知らないです。。</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>電子版は<a href="https://sites.google.com/view/nishimori/open_access_books">著者の先生のHP のページ</a>より無料DL できます <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><a class="u-url" href="/blog/physics/associative_memory" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">今月の物置</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">今月の新刊（@skrbcr）</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/skrbcr"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">skrbcr</span></a></li><li><a href="https://www.twitter.com/skrbcr"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">skrbcr</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>物理＋プログラミング＋その他</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
